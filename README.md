# ğŸš€ MLOps Portfolio â€” Pipeline ML End-to-End

> Projet dÃ©montrant une maÃ®trise complÃ¨te du cycle de vie d'un modÃ¨le ML en production :
> entraÃ®nement, dÃ©ploiement, monitoring de drift et CI/CD automatisÃ©.

![CI/CD](https://github.com/TON_USERNAME/mlops-portfolio/actions/workflows/ci.yml/badge.svg)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-green)
![MLflow](https://img.shields.io/badge/MLflow-2.10-orange)
![Docker](https://img.shields.io/badge/Docker-ready-blue)

## ğŸ¯ Objectif

Ce projet implÃ©mente un systÃ¨me MLOps complet, couvrant :
- **EntraÃ®nement** : pipeline sklearn avec tracking MLflow
- **DÃ©ploiement** : API REST FastAPI containerisÃ©e avec Docker
- **Monitoring** : dÃ©tection de data drift en temps rÃ©el (z-score)
- **CI/CD** : GitHub Actions â€” tests â†’ validation modÃ¨le â†’ deploy

## ğŸ—ï¸ Architecture

```
mlops-portfolio/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # API FastAPI (predict, train, metrics, drift)
â”‚   â””â”€â”€ monitoring.py    # DriftDetector + MetricsLogger
â”œâ”€â”€ model/
â”‚   â””â”€â”€ train.py         # Pipeline sklearn + MLflow tracking
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_all.py      # Tests unitaires & intÃ©gration (pytest)
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml       # Pipeline CI/CD GitHub Actions
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

## âš¡ Lancement rapide

```bash
# 1. Clone & install
git clone https://github.com/TON_USERNAME/mlops-portfolio
cd mlops-portfolio
pip install -r requirements.txt

# 2. Lancer l'API
uvicorn app.main:app --reload

# 3. Tester l'API
curl http://localhost:8000/docs   # Swagger UI interactif

# 4. Lancer les tests
pytest tests/ -v

# 5. Voir les runs MLflow
mlflow ui
```

## ğŸ³ Docker

```bash
docker build -t mlops-portfolio .
docker run -p 8000:8000 mlops-portfolio
```

## ğŸ“¡ Endpoints API

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/` | Info API |
| GET | `/health` | SantÃ© du service |
| POST | `/predict` | PrÃ©diction + dÃ©tection drift |
| POST | `/train` | Lance un entraÃ®nement + log MLflow |
| GET | `/metrics` | MÃ©triques de production (latence, drift) |
| GET | `/drift` | Rapport de drift dÃ©taillÃ© |

### Exemple de prÃ©diction

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [0.1, -0.5, 1.2, 0.3, -0.8, 0.6, -0.2, 0.9, -0.4, 0.7]}'
```

```json
{
  "prediction": 1.0,
  "confidence": 0.87,
  "model_version": "1.0.0",
  "latency_ms": 3.42,
  "drift_alert": false,
  "timestamp": "2025-02-13T14:32:01"
}
```

## ğŸ” DÃ©tection de Drift

Le systÃ¨me compare la distribution des requÃªtes en temps rÃ©el avec les donnÃ©es d'entraÃ®nement via un **z-score**. Si la moyenne glissante s'Ã©carte de plus de 2.5 Ã©carts-types de la rÃ©fÃ©rence, une alerte est dÃ©clenchÃ©e automatiquement.

```
GET /drift â†’ { "drift_count": 3, "drift_rate_pct": 1.2, "status": "stable" }
```

## ğŸ“Š MLflow Tracking

Chaque entraÃ®nement logue automatiquement :
- ParamÃ¨tres du modÃ¨le
- MÃ©triques : Accuracy, Precision, Recall, F1, AUC-ROC, CV scores
- Le modÃ¨le sÃ©rialisÃ©

```bash
mlflow ui  # â†’ http://localhost:5000
```

## ğŸ§ª Tests

```bash
pytest tests/ -v --cov=app --cov=model
```

Couverture : DriftDetector, MetricsLogger, Pipeline ML, Endpoints API.

## ğŸ› ï¸ Stack Technique

| Composant | Technologie |
|-----------|-------------|
| API | FastAPI + Uvicorn |
| ML | Scikit-learn (GBM, RF, LR) |
| Tracking | MLflow |
| SÃ©rialisation | Joblib |
| Tests | Pytest |
| CI/CD | GitHub Actions |
| Container | Docker |
| Deploy | Railway / Render |

## ğŸ‘¤ Auteur

**Rayan MALKI** â€” Ã‰tudiant M1 Data & IA
- GitHub: [@Rayanmlk](https://github.com/Rayanmlk)
- LinkedIn: [rayan malki](https://www.linkedin.com/in/rayan-malki/)
